# Classification-of-Tweets
## Introduction
* Sentiment analysis is the process of analyzing text to determine the emotional tone or sentiment of the text. This is a very useful tool in various fields like marketing, customer service, and politics. 
* There are two main approaches to sentiment analysis: **rule-based** and **machine learning-based**. 
* Rule-based methods use pre-defined rules and lexicons to determine the sentiment of the text. 
* On the other hand, machine learning-based methods use algorithms to learn from data and predict the sentiment.
## Methodology
The goal of this project is to investigate if a deep learning model can replicate the sentiment analysis scores generated by a well-known rule-based sentiment analysis package called "Sentiment Intensity Analyzer" from the NLTK package. To achieve this, we will use snscrape package to scrape 1000 tweets or more for any person or organization we are interested in from Twitter. We will then use the NLTK package to generate four sentiment scores for each tweet: positive, negative, neutral, and composite. These scores will be used as our target variables for our deep learning model, which will predict them based solely on the words in the tweets.

  1. **Scraping Tweets:** Scraped 4000 recent tweets tweeted by Joe Biden and Elon Musk using TwitterSearchScraper function from Snscrape package.
  2. **Preprocessing:** Preprocessed the Tweets by lowercasing all letters, filtering all non-alphanumeric characters, removing hashtags, mentions, punctuations, links, stopwords. Then tokenized the tweets using word_tokenize function from nltk and lemmatized using WordNetLemmatizer from nltk.
  3. **Sentiment Analysis:** Used the NLTK SentimentIntensityAnalyzer() to generate four sentiment scores: positive, negative, neutral, and compound
  4. **Build Neural Networks:** Built 4 deep learning models with 4 output nodes, one for each sentiment score. Trained the models on the preprocessed tweets and the sentiment scores generated by the SentimentIntensityAnalyzer(). Used a validation set to monitor the model's performance and fine-tuned the hyperparameters to improve the model's accuracy. Evaluated the performance of the deep learning model on test set of tweets.
  5. **Model Comparison:** Compared the predicted sentiment scores with the scores generated by the SentimentIntensityAnalyzer() to determine the accuracy of the model
## Word Cloud
  ![image](https://user-images.githubusercontent.com/48169929/226206741-dc138c95-79b3-40fc-8bdd-03513aa8aea1.png)
## Model Architectures
### Model-1: CountVectorizer with Dense Neural Network
  ![image](https://user-images.githubusercontent.com/48169929/226206703-d7dbb38b-4e4f-4378-a61f-830faa78ebd7.png)
### Model-2: TfidfVectorizer() with Dense Neural Network
  ![image](https://user-images.githubusercontent.com/48169929/226206685-c49b28c5-e809-459c-a892-da620c6c7b28.png)
### Model-3: Flattened Word Embeddings with Dense Neural Network
  ![image](https://user-images.githubusercontent.com/48169929/226206669-38140c98-d643-4875-8fe0-333fa974ce91.png)
### Model-4: Advanced Neural Network
  ![image](https://user-images.githubusercontent.com/48169929/226206654-a87a0174-ba55-427a-b89e-71cf835ca6aa.png)
## Model Comparisons
### RMSE
  ![image](https://user-images.githubusercontent.com/48169929/226206316-73b7dcc5-1dcc-435b-8f4d-54fd89d148ee.png)
### MAE
  ![image](https://user-images.githubusercontent.com/48169929/226206336-f83f867b-1044-42e9-bc88-7e8f5a82b28a.png)
* We have observed that for the Negative score, CountVectorizer() or TfidfVectorizer() are performing better that other models.
* We have observed that for the Neutral score, CountVectorizer() is performing better than other models.
* We have observed that for the Positive score, CountVectorizer() or TfidfVectorizer() are performing better that other models.
* We have observed that for the Compound score, CountVectorizer() is 
performing better than other models.
* We also observed that MAE for Positive and Negative Scores is less when compared to the MAE values for the Neutral and Compound scores.
## Conclusion
* From the RMSE and MAE values for the test data we can see that CountVectorizer() and TfidfVectorizer() have almost same values. 
* And also CountVectorizer() and TfidfVectorizer() are best models out of the 4 models as they have less RMSE and MAE values with good R squared values.
* Also these models are predicting Negative Score and Positive score more accurately than Neutral and compound Scores.
* Changing the hyper parameters also helped slightly but not to our expectations.
* Coming to the architecture, we have seen some improvements as we changed it.
* As we increase the number of layers, the performance has increased but not so high.
* Adding dropouts after NN layers helped to increase the R squared but not as expected. Still it is overfitting, but it helped.
* Reducing the patience value, batch size and limiting the batch size also helped in the improvement.
* Since, the data is very small, adding SimpleRNN layers to the model architecture gave slight improvement.
* The primary problem here is that the data we have is very small. It is only 4k tweets. But, the model is sightly better when the data size is increased from 2k to 4k tweets.
* So, increasing the data largely would improve the model performance in capturing the sentiments. 
* We can say that Deep learning models are able to steal the logic of the nltk SentimentIntensityAnalyzer() for atleast positive and negative scores.
* Rule-based models blindly follows more advanced rules. Deep-learning models doesn’t blindly tag keywords or ‘rules’. It infers meaning based on patterns between words and the wider context of the sentence and paragraph they sit within. 
* Deep learning models require more data as compared to rule-based models. Rule-based models can operate with simple basic information and data. However, deep learning models require full demographic data details.
